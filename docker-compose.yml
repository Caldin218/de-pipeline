version: '3.8'

x-airflow-common: &airflow-common
  env_file:
    - .env
  environment:
    - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    - AIRFLOW__CORE__FERNET_KEY=${FERNET_KEY}  
    - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=false
    - AIRFLOW__CORE__LOAD_EXAMPLES=false
    - SLACK_WEBHOOK_URL=${SLACK_WEBHOOK_URL}
    # ✅ thêm heartbeat tránh bị kill
    - AIRFLOW__SCHEDULER__TASK_HEARTBEAT_SEC=60
    - AIRFLOW__SCHEDULER__SCHEDULER_HEARTBEAT_SEC=60
    - AIRFLOW__CORE__KILLED_TASK_CLEANUP_TIME=120
  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - airflow-logs:/opt/airflow/logs
    - ./airflow/plugins:/opt/airflow/plugins
    - ./pipelines:/opt/airflow/dags/pipelines 
    - ./data:/app/data
    - ./jars:/opt/bitnami/spark/extra-jars
    - ./configs:/opt/airflow/configs
    - ./spark-events:/tmp/spark-events
    - /home/ubuntu/de-pipeline/docker/airflow/entrypoint.sh:/entrypoint.sh
  networks:
    - de-network
  restart: unless-stopped

services:
  postgres:
    image: postgres:14
    container_name: postgres
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data
    networks:
      - de-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow -d airflow"]
      interval: 10s
      timeout: 5s
      retries: 5

  airflow-webserver:
    <<: *airflow-common
    build:
     context: .
     dockerfile: Dockerfile.airflow
    image: de-airflow-webserver:2.8.1
    container_name: airflow-webserver
    entrypoint: ["/entrypoint.sh"]
    command: webserver
    user: "50000:0"
    ports:
      - "8080:8080"
    environment:
      - AIRFLOW__WEBSERVER__WEB_SERVER_HOST=0.0.0.0
    networks:
      de-network:
        aliases:
          - spark-master   # ✅ thêm alias để resolve được spark-master
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/health || exit 1"]
      interval: 30s
      retries: 5
    depends_on:
      postgres:
        condition: service_healthy

  airflow-scheduler:
    <<: *airflow-common
    build:
     context: .
     dockerfile: Dockerfile.airflow
    image: de-airflow-webserver:2.8.1
    container_name: airflow-scheduler
    entrypoint: ["/entrypoint.sh"]
    command: scheduler
    user: "50000:0"
    networks:
      de-network:
        aliases:
          - spark-master   # ✅ thêm alias để resolve được spark-master
    depends_on:
      postgres:
        condition: service_healthy

  spark-master:
    build:
      context: .
      dockerfile: ./docker/spark/Dockerfile  
    image: de-spark:3.5.0
    container_name: spark-master
    hostname: spark-master 
    env_file:
      - .env
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8081
      - SPARK_EXTRA_CLASSPATH=/opt/bitnami/spark/extra-jars/*
      - SPARK_EVENTLOG_ENABLED=true
      - SPARK_EVENTLOG_DIR=file:///tmp/spark-events
    ports:
      - "7077:7077"
      - "8081:8081"
    volumes:
      - ./jars:/opt/bitnami/spark/extra-jars
      - ./data:/app/data
      - ./pipelines:/opt/airflow/dags/pipelines
      - ./configs:/opt/airflow/configs
      - ./spark-events:/tmp/spark-events
    networks:
      - de-network
    restart: unless-stopped

  spark-worker:
    build:
      context: .
      dockerfile: ./docker/spark/Dockerfile
    image: de-spark:3.5.0
    container_name: spark-worker
    env_file:
      - .env
    environment:
      - SPARK_MODE=worker
      - SPARK_WORKER_WEBUI_PORT=8082
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_EXTRA_CLASSPATH=/opt/bitnami/spark/extra-jars/*
      - SPARK_HISTORY_OPTS=-Dspark.history.fs.logDirectory=/tmp/spark-events -Dspark.history.ui.port=18080
      - SPARK_EVENTLOG_ENABLED=true
      - SPARK_EVENTLOG_DIR=file:///tmp/spark-events
    ports:
      - "8082:8082"
    depends_on:
      - spark-master
    volumes:
      - ./jars:/opt/bitnami/spark/extra-jars
      - ./data:/app/data
      - ./pipelines:/opt/airflow/dags/pipelines
      - ./configs:/opt/airflow/configs
      - ./spark-events:/tmp/spark-events
    networks:
      - de-network
    restart: unless-stopped

  spark-history:
    image: bitnami/spark:3.5.0
    container_name: spark-history
    environment:
      - SPARK_HISTORY_OPTS=-Dspark.history.fs.logDirectory=/tmp/spark-events -Dspark.history.ui.port=18080
    command: >
      bash -c "
        /opt/bitnami/spark/bin/spark-class org.apache.spark.deploy.history.HistoryServer
      "
    ports:
      - "18080:18080"
    volumes:
      - ./spark-events:/tmp/spark-events
    networks:
      - de-network
    restart: unless-stopped

volumes:
  pgdata:
  airflow-logs:

networks:
  de-network:
    driver: bridge